磁盘I/O的三种方式对比：标准I/O、直接 I/O、mmap

【北桥集成的DMA模块，大大降低了CPU轮询总线中外围IO设备的时间占用，解放CPU的时间片，增加CPU利用率】

使用直接IO的考量

O_DIRECT 和 RAW设备最根本的区别是O_DIRECT是基于文件系统的，也就是在应用层来看，其操作对象是文件句柄，内核和文件层来看，其操作是基于inode和数据块，这些概念都是和ext2/3的文件系统相关，写到磁盘上最终是ext3文件。
而RAW设备写是没有文件系统概念，操作的是扇区号，操作对象是扇区，写出来的东西不一定是ext3文件（如果按照ext3规则写就是ext3文件）。
一般基于O_DIRECT来设计优化自己的文件模块，是不满系统的cache和调度策略，自己在应用层实现这些，来制定自己特有的业务特色文件读写。但是写出来的东西是ext3文件，该磁盘卸下来，mount到其他任何linux系统上，都可以查看。
而基于RAW设备的设计系统，一般是不满现有ext3的诸多缺陷，设计自己的文件系统。自己设计文件布局和索引方式。举个极端例子：把整个磁盘做一个文件来写，不要索引。这样没有inode限制，没有文件大小限制，磁盘有多大，文件就能多大。这样的磁盘卸下来，mount到其他linux系统上，是无法识别其数据的。
两者都要通过驱动层读写；在系统引导启动，还处于实模式的时候，可以通过bios接口读写raw设备。

1.1 标准I/O

大多数文件系统的默认I/O操作都是标准I/O。在Linux的缓存I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。
读操作：操作系统检查内核的缓冲区有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。
写操作：将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用了sync等同步命令。
缓存I/O的优点：1）在一定程度上分离了内核空间和用户空间，保护系统本身的运行安全；2）可以减少读盘的次数，从而提高性能。
缓存I/O的缺点：数据在传输过程中需要在应用程序地址空间和缓存之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。
page cache 回写机制:当更改过的内存占用全部内存达到一个百分比时，或字节数到一定数量后，或一定周期后，会回写磁盘
page cache 与文件系统的inode相关联
https://zhuanlan.zhihu.com/p/35277219

read() disk -> kernel(call) -> user(varable)
write() user(varable) -> kernel(call) -> disk
need sync or fsync to flush in disk


1.2直接I/O

直接IO就是应用程序直接访问磁盘数据，而不经过内核缓冲区，这样做的目的是减少一次从内核缓冲区到用户程序缓存的数据复制。比如说数据库管理系统这类应用，它们更倾向于选择它们自己的缓存机制，因为数据库管理系统往往比操作系统更了解数据库中存放的数据，数据库管理系统可以提供一种更加有效的缓存机制来提高数据库中数据的存取性能。
直接IO的缺点：如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘加载，这种直接加载会非常耗时。通常直接IO与异步IO结合使用，会得到比较好的性能。（异步IO：当访问数据的线程发出请求之后，线程会接着去处理其他事，而不是阻塞等待）

忽略内核态缓存 buffer 用户态维护缓存状态
user(buffer) -> filesystem(no buffer) -> io-queue(schedule) -> hard disk or solid state disk


1.3 mmap

mmap是指将硬盘上文件的位置与进程逻辑地址空间中一块大小相同的区域一一对应，当要访问内存中一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据在用户空间和内核空间之间的拷贝操作。当大量数据需要传输的时候，采用内存映射方式去访问文件会获得比较好的效率。
使用内存映射文件处理存储于磁盘上的文件时，将不必再对文件执行I/O操作，这意味着在对文件进行处理时将不必再为文件申请并分配缓存，所有的文件缓存操作均由系统直接管理，由于取消了将文件数据加载到内存、数据从内存到文件的回写以及释放内存块等步骤，使得内存映射文件在处理大数据量的文件时能起到相当重要的作用。

mmap的优点：
减少系统调用。我们只需要一次 mmap() 系统调用，后续所有的调用像操作内存一样，而不会出现大量的 read/write 系统调用而可以绕过系统缓存。
减少数据拷贝。普通的 read() 调用，数据需要经过两次拷贝；而 mmap 只需要从磁盘拷贝一次就可以了，并且由于做过内存映射，也不需要再拷贝回用户空间。
可靠性高。mmap 把数据写入页缓存后，跟缓存 I/O 的延迟写机制一样，可以依靠内核线程定期写回磁盘。但是需要提的是，mmap 在内核崩溃、突然断电的情况下也一样有可能引起内容丢失，当然我们也可以使用 msync来强制同步写。

缺点：
虚拟内存增大。mmap在物理内存上使用通用的缺页中断加载，但虚拟内存空间中需要一段连续的地址已保证操作，文件过大，容易造成OOM崩溃，且如果内存碎片过多，极有可能映射失败
磁盘延迟。mmap 通过缺页中断向磁盘发起真正的磁盘 I/O，所以如果我们当前的问题是在于磁盘 I/O 的高延迟，那么用 mmap() 消除小小的系统调用开销是杯水车薪的。


http://blog.chinaunix.net/uid-27105712-id-3270102.html


在传统的数据库中对数据库文件的相关操作为什么不用mmap？传统数据库对datafile的读写大部分是通过open系统调用加O_DIRECT标志。使用O_DIRECT标志可以跳过kernel的page cache而直接与block device(如磁盘)打交道，与普通的read/write相比少了一层缓存(page cache)，数据库开发者通过实现在用户层的高效缓存来达到提高效率目的。但是这带来很大的复杂性问题，首先使用O_DIRECT的话，就必须以页为单位进行I/O，而且既然放弃kernel的提供page cache以及相关的缓存策略，那么意味着是想通过O_DIRECT提供自己的更好的缓存策略，这个往往是很困难的。在Linus看来，“O_DIRECT是一个非常糟糕的接口，目前只为数据库开发者保留，其他人使用都属于脑残的行为”。数据库开发者想通过直接与device打交道(简单说，他们觉得能比OS干得更好)，来提高IO性能，例如提供更适合数据库的缓存策略(如LIRS缓存算法)。例如Innodb中通过配置文件的形式提供了两种方式读写数据文件，一种是传统的read/write读写，一种是O_DIRECT访问。一般情况下O_DIRECT的性能要高。

http://useless-factor.blogspot.com/2011/05/why-not-mmap.html
http://nineright.github.io/
